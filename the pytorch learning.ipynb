{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42b2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"/Users/xieh/Desktop/CC/IT100/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e131323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Type: <class 'numpy.ndarray'>\n",
      "Array Shape: (2, 3)\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy array\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "first_array = np.array(array) # 2x3 array\n",
    "print(\"Array Type: {}\".format(type(first_array))) # type\n",
    "print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\n",
    "print(first_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc0a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Type: <built-in method type of Tensor object at 0x100f524f0>\n",
      "Array Shape: torch.Size([2, 3])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# pytorch array\n",
    "tensor = torch.Tensor(array)\n",
    "print(\"Array Type: {}\".format(tensor.type)) # type\n",
    "print(\"Array Shape: {}\".format(tensor.shape)) # shape\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e781d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# numpy ones\n",
    "print(\"Numpy {}\\n\".format(np.ones((2,3))))\n",
    "\n",
    "# pytorch ones\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e434a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[0.6615317  0.92257202]\n",
      " [0.60407021 0.00450364]]\n",
      "\n",
      "tensor([[0.6615, 0.9226],\n",
      "        [0.6041, 0.0045]], dtype=torch.float64)\n",
      "\n",
      "<class 'numpy.ndarray'> [[0.6615317  0.92257202]\n",
      " [0.60407021 0.00450364]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random numpy array\n",
    "array = np.random.rand(2,2)\n",
    "print(\"{} {}\\n\".format(type(array),array))\n",
    "\n",
    "# from numpy to tensor\n",
    "from_numpy_to_tensor = torch.from_numpy(array)\n",
    "print(\"{}\\n\".format(from_numpy_to_tensor))\n",
    "\n",
    "# from tensor to numpy\n",
    "tensor = from_numpy_to_tensor\n",
    "from_tensor_to_numpy = tensor.numpy()\n",
    "print(\"{} {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31694d2",
   "metadata": {},
   "source": [
    "Resize: view()\n",
    "a and b are tensor.\n",
    "Addition: torch.add(a,b) = a + b\n",
    "Subtraction: a.sub(b) = a - b\n",
    "Element wise multiplication: torch.mul(a,b) = a * b\n",
    "Element wise division: torch.div(a,b) = a / b\n",
    "Mean: a.mean()\n",
    "Standart Deviation (std): a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d6fca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.Size([9])tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "Addition: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "Subtraction: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Element wise multiplication: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Element wise division: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Mean: 3.0\n",
      "std: 1.5811388492584229\n"
     ]
    }
   ],
   "source": [
    "# create tensor \n",
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n\",tensor)\n",
    "\n",
    "# Resize\n",
    "print(\"{}{}\\n\".format(tensor.view(9).shape,tensor.view(9)))\n",
    "\n",
    "# Addition\n",
    "print(\"Addition: {}\\n\".format(torch.add(tensor,tensor)))\n",
    "\n",
    "# Subtraction\n",
    "print(\"Subtraction: {}\\n\".format(tensor.sub(tensor)))\n",
    "\n",
    "# Element wise multiplication\n",
    "print(\"Element wise multiplication: {}\\n\".format(torch.mul(tensor,tensor)))\n",
    "\n",
    "# Element wise division\n",
    "print(\"Element wise division: {}\\n\".format(torch.div(tensor,tensor)))\n",
    "\n",
    "# Mean\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(\"Mean: {}\".format(tensor.mean()))\n",
    "\n",
    "# Standart deviation (std)\n",
    "print(\"std: {}\".format(tensor.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f730fb7",
   "metadata": {},
   "source": [
    "Variables\n",
    "\n",
    "It accumulates gradients.\n",
    "We will use pytorch in neural network. And as you know, in neural network we have backpropagation where gradients are calculated. Therefore we need to handle gradients. If you do not know neural network, check my deep learning tutorial first because I will not explain detailed the concepts like optimization, loss function or backpropagation.\n",
    "Deep learning tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n",
    "Difference between variables and tensor is variable accumulates gradients.\n",
    "We can make math operations with variables, too.\n",
    "In order to make backward propagation we need variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cada0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import variable from pytorch library\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# define variable\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58394111",
   "metadata": {},
   "source": [
    "Assume we have equation y = x^2\n",
    "Define x = [2,4] variable\n",
    "After calculation we find that y = [4,16] (y = x^2)\n",
    "Recap o equation is that o = (1/2)sum(y) = (1/2)sum(x^2)\n",
    "deriavative of o = x\n",
    "Result is equal to x so gradients are [2,4]\n",
    "Lets implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df3e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y =   tensor([ 4., 16.], grad_fn=<PowBackward0>)\n",
      " o =   tensor(10., grad_fn=<MulBackward0>)\n",
      "gradients:  tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "#lets make basic backward propagation\n",
    "# we have an equation that is y = x^2\n",
    "array = [2,4]\n",
    "tensor = torch.Tensor(array)\n",
    "x = Variable(tensor, requires_grad = True)\n",
    "y = x**2\n",
    "print(\" y =  \",y)\n",
    "\n",
    "# recap o equation o = 1/2*sum(y)\n",
    "o = (1/2)*sum(y)\n",
    "print(\" o =  \",o)\n",
    "\n",
    "# backward\n",
    "o.backward() # calculates gradients\n",
    "\n",
    "# As I defined, variables accumulates gradients. In this part there is only one variable x.\n",
    "# Therefore variable x should be have gradients\n",
    "# Lets look at gradients with x.grad\n",
    "print(\"gradients: \",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c3a6b7",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=IHZwWFHWa-w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba7b99",
   "metadata": {},
   "source": [
    "Prepare Dataset\n",
    "We use MNIST dataset.\n",
    "There are 28*28 images and 10 labels from 0 to 9\n",
    "Data is not normalized so we divide each image to 255 that is basic normalization for images.\n",
    "In order to split data, we use train_test_split method from sklearn library\n",
    "Size of train data is 80% and size of test data is 20%.\n",
    "Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n",
    "batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n",
    "epoch: 1 epoch means training all samples one time.\n",
    "In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate:\n",
    "training data 1 times = training 33600 sample (because data includes 33600 sample)\n",
    "But we split our data 336 groups(group_size = batch_size = 100) our data\n",
    "Therefore, 1 epoch(training data only once) takes 336 iteration\n",
    "We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n",
    "TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n",
    "DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n",
    "Visualize one of the images in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56976171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "542c30d1",
   "metadata": {},
   "source": [
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2ac67",
   "metadata": {},
   "source": [
    "CNN\n",
    "https://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9e62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e0d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70dfa91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
